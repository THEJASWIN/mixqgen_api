{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "\n",
    "def scrap(inputLink):\n",
    "\n",
    "    web = DesiredCapabilities.CHROME\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    driver = webdriver.Remote(\n",
    "        command_executor='https://selenium-chrome-2o4gzaw5dq-el.a.run.app/wd/hub',\n",
    "        desired_capabilities=web,\n",
    "        options=options)\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    driver.delete_all_cookies()\n",
    "    allContent = []\n",
    "    driver.get(inputLink)\n",
    "\n",
    "    try:\n",
    "        article = driver.find_elements_by_tag_name('section')\n",
    "        article_title = driver.find_element_by_tag_name('h1')\n",
    "\n",
    "        for data in range(len(article)):\n",
    "            temp_data = article[data].find_elements_by_tag_name(\"p\")\n",
    "            for text in temp_data:\n",
    "                allContent.append(text.text)\n",
    "        allPara = \"\".join(allContent)\n",
    "\n",
    "        '''\n",
    "            The content is been formatted to JSON\n",
    "        '''\n",
    "        \n",
    "        medium = {\"Title\":article_title.text,\"Content\":allPara}\n",
    "        output = json.dumps(medium,indent=2)\n",
    "        output_json = json.loads(output)\n",
    "        '''\n",
    "            Returns the JSON data\n",
    "        '''\n",
    "        return output_json\n",
    "    finally:\n",
    "        print(\"......Exiting Medium.....\")\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = input()\n",
    "out = scrap(link)\n",
    "\n",
    "\n",
    "split_sentence = re.split('min read',out[\"Content\"])[1:]\n",
    "result = \"\".join(split_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mout\u001b[49m) \n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# len(out[\"Content\"])\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# https://medium.com/@rishit.dagli/get-started-with-tensorflow-and-deep-learning-part-1-72c7d67f99fc\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(result)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "print(out) \n",
    "# len(out[\"Content\"])\n",
    "# https://medium.com/@rishit.dagli/get-started-with-tensorflow-and-deep-learning-part-1-72c7d67f99fc\n",
    "# len(result)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = :\"Denny1 day ago·4 min readWhen we start off with creating a web application, we don’t care about how performant it is (and we shouldn’t to be honest). Our aim is to get a basic POC done and see if it gets some audience. This approach fits well until your user base grows beyond a number. Let’s see what happens when your user base is growing.Your server receives 5 requests every second. You have a reverse proxy designed to forward those requests to your HTTP server. You have configured your HTTP server to forward these requests to your application, which in turn processes the requests one by one and returns a response back. If you are working with a framework like Django which uses Python which has limitations when it comes to running parallel processes, this approach can cause issues. If you are one of the application developers and hasn’t done much to optimise your endpoints, then trust me, your application is entirely dependent on the amazing architecture of your HTTP server and reverse proxy!When a lot of requests pour into your service, optimising your application code by reducing the number of loops or substituting operations with inbuilt functions can only go so far. Asynchronous programming to the rescue! It’s a new paradigm of programming. It’s not intuitive. When I learned it myself, I found it really hard to understand. But it’s effective!Let’s say a user is creating a profile on your website. Now, you have to send a mail to their email ID asking for confirmation. But when the server tries to send a mail, it has to communicate with the SMTP server, wait for it to respond, send the message, wait for the acknowledgement.See how much waiting is done by the server? What if it can execute other tasks during this time instead of being idle?Now, that would decrease the overall time required to complete the process. Hence, in such scenarios, we break from our pattern of executing the code line by line and use asynchronous functions to speed up the execution.Another way of thinking about async is to understand what are blocking and non-blocking tasks. In the backend, when you are requesting some data from the database, the process running your program is waiting for the response from the database and sitting idle. In this case, it’s blocking the execution of further code.While certain languages like Javascript have AJAX calls by default async, in other languages like Python, you will have to make your requests async deliberately.Some of the scenarios where you would consider doing async operation:Every language has its own way of doing it. While javascript has async/await keywords to do it, Java has threads and Python has both. What method works for you best in your scenario or language is something you need to explore. For e.g. if you are programming in python, you don’t have to start coding async functions from scratch.If you want to dive deep into how to write asynchronous programs from scratch you can check out this tutorial by Tech with Tim — VideoI am not getting into the discussion of concurrency and parallelism right now. That’s a topic for another post. Stay tuned!Asynchronous programming is not suitable if your tasks are CPU intensive, since async programming leverages the idle time of CPU when it’s performing I/O operations.So if you are using a deep learning model to analyse an image, it wouldn’t really do much to scale your application.Get comfortable with code that doesn’t execute linearly. It’s gonna take some effort but it’s also going to help you make insanely scalable applications\n",
    "# When we start off with creating a web application, we don’t care about how performant it is and we shouldn’t to be honest Our aim is to get a basic POC done and see if it gets some audience. This approach fits well until your user base grows beyond a number. Let’s see what happens when your user base is growing.Your server receives 5 requests every second. You have a reverse proxy designed to forward those requests to your HTTP server. You have configured your HTTP server to forward these requests to your application, which in turn processes the requests one by one and returns a response back. If you are working with a framework like Django which uses Python which has limitations when it comes to running parallel processes, this approach can cause issues. If you are one of the application developers and hasn’t done much to optimise your endpoints, then trust me, your application is entirely dependent on the amazing architecture of your HTTP server and reverse proxy!When a lot of requests pour into your service, optimising your application code by reducing the number of loops or substituting operations with inbuilt functions can only go so far. Asynchronous programming to the rescue! It’s a new paradigm of programming. It’s not intuitive. When I learned it myself, I found it really hard to understand. But it’s effective!Let’s say a user is creating a profile on your website. Now, you have to send a mail to their email ID asking for confirmation. But when the server tries to send a mail, it has to communicate with the SMTP server, wait for it to respond, send the message, wait for the acknowledgement.See how much waiting is done by the server? What if it can execute other tasks during this time instead of being idle?Now, that would decrease the overall time required to complete the process. Hence, in such scenarios, we break from our pattern of executing the code line by line and use asynchronous functions to speed up the execution.Another way of thinking about async is to understand what are blocking and non-blocking tasks. In the backend, when you are requesting some data from the database, the process running your program is waiting for the response from the database and sitting idle. In this case, it’s blocking the execution of further code.While certain languages like Javascript have AJAX calls by default async, in other languages like Python, you will have to make your requests async deliberately.Some of the scenarios where you would consider doing async operation:Every language has its own way of doing it. While javascript has async/await keywords to do it, Java has threads and Python has both. What method works for you best in your scenario or language is something you need to explore. For e.g. if you are programming in python, you don’t have to start coding async functions from scratch.If you want to dive deep into how to write asynchronous programs from scratch you can check out this tutorial by Tech with Tim — VideoI am not getting into the discussion of concurrency and parallelism right now. That’s a topic for another post. Stay tuned!Asynchronous programming is not suitable if your tasks are CPU intensive, since async programming leverages the idle time of CPU when it’s performing I/O operations.So if you are using a deep learning model to analyse an image, it wouldn’t really do much to scale your application.Get comfortable with code that doesn’t execute linearly. It’s gonna take some effort but it’s also going to help you make insanely scalable applications.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "text = \"Denny1 day ago·4 min readWhen we start off with creating a web application\"\n",
    "res = len(text.split())\n",
    "len(text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping using Section\n",
      "Error in Section\n",
      "True\n",
      "Scraping using X_path\n",
      "Kannan\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "# Error = None\n",
    "# try:\n",
    "#     # print(\"Scraping using Section\")\n",
    "#     # data = \"Code_1\"\n",
    "#     # new = data/2        \n",
    "# except Exception as error:\n",
    "#     print(\"Error in Section\")\n",
    "#     Error = True\n",
    "#     print(Error)\n",
    "\n",
    "#     if Error==True:\n",
    "#         try:\n",
    "#             # #cannot be scraped\n",
    "#             # #POST\n",
    "#             # print(\"Scraping using X_path\")\n",
    "#             # data_1 = \"Kannan\"\n",
    "#             # print(data_1)\n",
    "#             # # new_1 = data_1+2\n",
    "#         except Exception as error:\n",
    "#             print(error.__class__)\n",
    "#             print(\"Cannot be scrapped\")\n",
    "#             #POST REQ\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "# finally:\n",
    "#     print(\"exit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "\n",
    "def scrap(inputLink):\n",
    "    Error = None\n",
    "    web = DesiredCapabilities.CHROME\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    driver = webdriver.Remote(\n",
    "        command_executor='http://localhost:4444/wd/hub',\n",
    "        desired_capabilities=web,\n",
    "        options=options)\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "    driver.delete_all_cookies()\n",
    "    allContent = []\n",
    "    driver.get(inputLink)\n",
    "\n",
    "    try:\n",
    "        print(\"Scrapping using SectionTag\")\n",
    "\n",
    "        article = driver.find_elements_by_tag_name('section')\n",
    "        article_title = driver.find_element_by_tag_name('h1')  \n",
    "\n",
    "        for data in range(len(article)):\n",
    "            temp_data = article[data].find_elements_by_tag_name(\"p\")\n",
    "            for text in temp_data:\n",
    "                allContent.append(text.text)\n",
    "        allPara = \"\".join(allContent)\n",
    "        split_sentence = re.split('min read',allPara)[1:]\n",
    "        result = \"\".join(split_sentence)\n",
    "        '''\n",
    "            The content is been formatted to JSON\n",
    "        '''\n",
    "        medium = {\"Title\":article_title.text,\"Content\":result}\n",
    "        output = json.dumps(medium,indent=2)\n",
    "        output_json = json.loads(output)\n",
    "        '''\n",
    "            Returns the JSON data\n",
    "        '''\n",
    "        return output_json\n",
    "\n",
    "    except Exception as error_1:\n",
    "        print(\"Sorry\",error_1.__class__,\"Occured\")\n",
    "        Error = True\n",
    "\n",
    "        if Error == True:\n",
    "            try:\n",
    "                print(\"Scrapping using XPath\")\n",
    "                \n",
    "                article = driver.find_elements_by_xpath('//*[@id=\"root\"]/div/div[3]/article/div/div/section[1]/div/div')\n",
    "                article_title = article[0].find_elements_by_tag_name(\"h1\")\n",
    "\n",
    "                for data in range(len(article)):\n",
    "                    temp_data = article[data].find_elements_by_tag_name(\"p\")\n",
    "                    for text in temp_data:\n",
    "                        allContent.append(text.text)\n",
    "\n",
    "                allPara = \"\".join(allContent)\n",
    "                '''\n",
    "                    The content is been formatted to JSON\n",
    "                '''\n",
    "                medium = {\"Title\":article_title[0].text,\"Content\":allPara}\n",
    "                output = json.dumps(medium,indent=4)\n",
    "                output_json = json.loads(output)\n",
    "                '''\n",
    "                    Returns the JSON data\n",
    "                '''\n",
    "                return output_json\n",
    "                \n",
    "            except Exception as error_2:\n",
    "                print(\"Cannot be Scrapped!\")\n",
    "                print(\"Sorry\",error_2.__class__,\"Occured\")\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    finally:\n",
    "        print(\"......Exiting Medium......\")\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping using SectionTag\n",
      "......Exiting Medium......\n",
      "{'Title': 'Get started with TensorFlow and Deep Learning', 'Content': \"All the code used here is available at the GitHub repository here.This is the first part of a series where I will be posting many blog posts about coding Machine Learning and Deep Learning algorithms. I believe in hands-on coding so we will have many exercises and demos which you can try yourself too. I would recommend you to play around with these exercises and change the hyper-parameters and experiment with the code. That is the best way to learn and code efficiently. If you don’t know what hyper-parameters are don’t worry. We will walk through Tensor Flow’s core capabilities and then also explore high-level API’s in TensorFlow and so on.If you are a person who is looking to get started with Tensor Flow, Machine Learning, and Deep Learning but don’t have any knowledge about Machine Learning you can do this.\\nIf you already know about Machine Learning and Deep Learning and don’t know about Tensor Flow you can follow these. As you might be using some other frameworks or libraries like Caffe, Theano, CNTK, Apple ML or PyTorch and might also be knowing about the maths behind it you can skip the sections marked OPTIONAL.\\nIf you know about the basics of Tensor Flow then you can skip some of the blog posts.No!!Not at all, you don’t need to install any software at all to complete the exercises and follow the code at all. We will be using Google Colaboratory. Colaboratory is a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use. Colaboratory works with most major browsers and is most thoroughly tested with the latest versions of Chrome, Firefox, and Safari. And what more, it is completely free to use. All Colaboratory notebooks are stored in Google Drive. Colaboratory notebooks can be shared just as you would with Google Docs or Sheets. Simply click the Share button at the top right of any Colaboratory notebook, or follow these Google Drive file sharing instructions. Before, you ask me Colaboratory is really reliable and fast too.However, you can also use a local development environment preferably Jupyter Notebooks but we will not be covering how to set up the local environment in these blogs.The answer to this question is that in this blog we will cover both. If you are using Colab notebooks add this code.Coding has been the bread and butter for developers since the dawn of computing. We’re used to creating applications by breaking down requirements into composable problems that can then be coded against. So for example, if we have to write an application that figures out a stock analytic, maybe the price divided by the ratio, we can usually write code to get the values from a data source, do the calculation and then return the result. Or if we’re writing a game we can usually figure out the rules. For example, if the ball hits the brick then the brick should vanish and the ball should rebound. But if the ball falls off the bottom of the screen then maybe the player loses their life.We can represent that with this diagram. Rules and data go in answers come out. Rules are expressed in a programming language and data can come from a variety of sources from local variables up to databases. Machine learning rearranges this diagram where we put answers in data in and then we get rules out. So instead of us as developers figuring out the rules when should the brick be removed, when should the player’s life end, or what’s the desired analytic for any other concept, what we will do is we can get a bunch of\\nexamples for what we want to see and then have the computer figure out the rules.So consider this example, activity recognition. If I’m building a device that detects if somebody is say walking and I have data about their speed, I might write code like this and if they’re running well that’s a faster speed so I could adapt my code to this and if they’re biking, well that’s not too bad either. I can adapt my code like this. But then I have to do golf recognition too, now my concept becomes broken. But not only that, doing it by speed alone of course is quite naive. We walk and run at different speeds uphill and downhill and other people walk and run at different speeds to us.The new paradigm is that I get lots and lots of examples and then I have labels on those examples and I use the data to say this is what walking looks like, this is what running looks like, this is what biking looks like and yes, even this is what golfing looks like. So, then it becomes answers and data in with rules being inferred by the machine. A machine learning algorithm then figures out the specific patterns in each set of data that determines the distinctiveness of each. That’s what’s so powerful and exciting about this programming paradigm. It’s more than just a new way of doing the same old thing. It opens up new possibilities that were infeasible to do before.So, now I am going to show you the basics of creating a neural network for doing this type of pattern recognition. A neural network is just a slightly more advanced implementation of machine learning and we call that deep learning. But fortunately it's actually very easy to code. So, we're just going to jump straight into deep learning. We'll start with a simple one and then we'll move on to one that does computer vision in about 10 lines of code.To show how that works, let’s take a look at a set of numbers and see if you can determine the pattern between them. Okay, here are the numbers.x = -1, 0, 1, 2, 3, 4y = -3, -1, 1, 3, 5, 7You easily figure this out y = 2x — 1You probably tried that out with a couple of other values and see that it fits. Congratulations, you’ve just done the basics of machine learning in your head.Okay, here’s our first line of code. This is written using Python and TensorFlow and an API in TensorFlow called keras. Keras makes it really easy to define neural networks. A neural network is basically a set of functions which can learn patterns.The simplest possible neural network is one that has only one neuron in it, and that’s what this line of code does. In keras we use the word Dense to define a layer of connected neurons. There is only one Dense here means that there is only one layer and there is only single unit in it so there is only one neuron. Successive layers in keras are defined in a sequence so the word Sequential . You define the shape of what's input to the neural network in the first and in this case the only layer, and you can see that our input shape is super simple. It's just one value. You've probably seen that for machine learning, you need to know and use a lot of math, calculus probability and the like. It's really good to understand that as you want to optimize your models but the nice thing for now about TensorFlow and keras is that a lot of that math is implemented for you in functions. There are two function roles that you should be aware of though and these are loss functions and optimizers.This lines defines that for us. So, lets understand what it is.Understand it like this the neural network has no idea of the relation between X and Y so it makes a random guess say y=x+3 . It will then use the data that it knows about, that’s the set of Xs and Ys that we’ve already seen to measure how good or how bad its guess was. The loss function measures this and then gives the data to the optimizer which figures out the next guess. So the optimizer thinks about how good or how badly the guess was done using the data from the loss function. Then the logic is that each guess should be better than the one before. As the guesses get better and better, an accuracy approaches 100 percent, the term convergence is used.Here we have used the loss function as mean squared error and optimizer as SGD or stochactic gradient descent.Our next step is to represent the known data. These are the Xs and the Ys that you saw earlier. The np.array is using a Python library called numpy that makes data representation particularly enlists much easier. So here you can see we have one list for the Xs and another one for the Ys. The training takes place in the fit command. Here we’re asking the model to figure out how to fit the X values to the Y values. The epochs equals 500 value means that it will go through the training loop 500 times. This training loop is what we described earlier. Make a guess, measure how good or how bad the guesses with the loss function, then use the optimizer and the data to make another guess and repeat this. When the model has finished training, it will then give you back values using the predict method.Here’s the code of what we talked about.So, what output do you except — 19, right?But when you try this in the workbook yourself you will see it gives me a value close to 19 not 19. Like you might receive-Why do you think this happens because the equation is y = 2x-1 .There are two main reasons:The first is that you trained it using very little data. There’s only six points. Those six points are linear but there’s no guarantee that for every X, the relationship will be Y equals 2X minus 1. There’s a very high probability that Y equals 19 for X equals 10, but the neural network isn’t positive. So it will figure out a realistic value for Y. The the second main reason. When using neural networks, as they try to figure out the answers for everything, they deal in probability. You’ll see that a lot and you’ll have to adjust how you handle answers to fit. Keep that in mind as you work through the code. Okay, enough talking. Now let’s get hands-on and write the code that we just saw and then we can run it.If you have used Jupyter before you will find Colab easy to use. If you have not used Colab before consider watching this intro to it.Now you know how to use Colab so let’s get started.If you are using Jupyter Notebooks in your local environment download the code file here.If you are using Colab follow the link here. You need to sign in with your Google account to run the code and receive a hosted run time.I recommend you to spend some time with the notebook and try changing the epochs and see the effect on loss and accuracy and experiment with the code.In this exercise you’ll try to build a neural network that predicts the price of a house according to a simple formula. So, imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc. How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc.Hint: Your network might work better if you scale the house price down. You don’t have to give the answer 400…it might be better to create something that predicts the number 4, and then your answer is in the ‘hundreds of thousands’ etc.Note: The maximum error allowed is 400 dollarsMake a new Colab Notebook and write your solution for this exercise and try it with some numbers the model has not seen.Wonderful, you just created a neural net all by yourself and some of you must be feeling like this (it’s perfectly ok)-Let’s see my solution to this.And this gives me the answer to the required tolerate limit.Here’s the Solution to this exercise — hereIf you want to try this out in Colab you can go to the link and click open on colab button.That was it, the basics. You even created a neural net all by yourself and made pretty accurate predictions too. Next, we will see about using some image classifiers with Tensor Flow so, stay tuned and explore till then.Some TensorFlow research papersNext Blog in this series hereHi everyone I am Rishit DagliTwitterWebsiteIf you want to ask me some questions, report any mistake, suggest improvements, give feedback you are free to do so by emailing me at —\"}\n"
     ]
    }
   ],
   "source": [
    "link = input()\n",
    "print(scrap(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafromMedium = \"Building a Docker image is generally considered trivial compared to developing other components of a ML system like data pipeline, model training, serving infra, etc. But an inefficient, bulky docker image can greatly reduce performance and can even bring down the serving infra.This blog aims to focus on building an ideal Docker image and not on its concept or benefits. I am assuming you have basic knowledge of a few topics wrt to Docker:There quite a few very good source for general best-practice like official docker guide, but I would like to keep this short and relevant to ML system based projecteg:Most of the time a ML system will be based on Python, so it critical building any Python-based Docker image efficiently. Let us go through them.2.1 Single StageFor demo purpose, I am using the following packages:After running the docker build command the size of docker image was 1.64 gb.2.2 Multi-StageComparing them, multi-stage docker image size is 1.61 gb and single-stage is 1.64 gb. Its an improvement (even it seems small though), a lot of things are going here, lets us try to understand in a nutshell.→Many libraries now come as pre-compiled .whl are wheel format from PyPi, which does not need any kind of compilation.→So does it mean, there is no scope of Multi-stage build for Python project? Absolutely yes!!! Not every package from PyPi are pre-compiled .whl format, many are all legacy tar.gz (tarballs compressed), which needs to be first compiled & here the Multi-stage build will work its charm.→ Also, Multi-stage is applicable is you are building a python package from source or using local package using setup.py, as again, they need to be compiled first.→ I would highly insist you to read this article from Real Python explaining what are wheels in python.→From the req.txt that I am using for the demo, only the above packages are not wheel format & also they are already very small in size. But if some packages are not pre-compiled wheel and large in size will end up wasting a lot of disk sizeBuilding a CPU based Docker image is not complex, but not the same case with building a GPU based docker. If not build appropriately, it can end up in humongous size. I will focus on practical and implementation part and not cover its theory part (as I think it is out of scope for this article).3.1 Understanding Pre-requisite→ Always use the same CUDA and cuDNN version in Docker image as present in the underlying host machine.→ Don’t blindly install latest tensorflow/pytorch library from PyPi. It is absolutely incorrect that any version of this both package will work with any version of CUDA, cuDNN. In fact, the combination of the latest version of both, tensorflow/pytorch with CUDA/cuDNN may not be compatible. Always test the combination in a development environment first.→ Docker hub of Nvidia has a lot of images, so understanding their tags and selecting the correct image is the most important building block. The description from official Nvidia docker hub is,We are only interested in base, runtime and not in devel (as we are targeting prod environment). How to select an exact specific tag? I’ll answer it in following sub-part.3.2 Single Stagei. Step 1: Check Version of CUDA and cuDNN of the underlying host machineii. Step 2: Select the Docker image based on step 1. So in my case, I have selected, ‘nvidia/cuda:10.1-cudnn7-runtime’. Why runtime? Because this is the one which includes both CUDA and cuDNN.iii. Step 3: Select correct version of tensorflow/pytorch which is compatible with this version of CUDA and cuDNN. In my case, it was tensorflow=2.20.iv. Cautionary step: The docker image from Nvidia might be older Ubuntu (18.04 or even 16.04) which will install python 3.6. So attention must be given here to check the compatibility of your project as well as external packages with python version. Anyways specific version can be installed from source.Note: As you can the Docker image from nvidia is based on ubuntu 18.04, I have to make a little additional adjustment to install tensorflow=2.2.0.3.3 Multi-stageNote: To make python 3.8 as default I have added some additional code, if this is not the case for you then you can avoid this hassle.i. Potential wasted space\\nii. Image efficiency scoreConclusion: The primary goal always must be minimal docker image size, since any docker image build for ML system will be always heavy. We should always follow all best practice especially Multi-stage build and versioning of packages. Last but also most important for gpu based images is to test the configuration on the dev environment.You can reach out to me via LinkedIn if any help is required.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775\n"
     ]
    }
   ],
   "source": [
    "res = len(datafromMedium.split())\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloud-trigger-dev-2o4gzaw5dq-el.a.run.app/question_link/update-status/61efd3115e85ff38c28daeb3\n",
      "<Response [200]>\n",
      "b'{\"message\":\"link status updated\"}'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API  = \"https://cloud-trigger-dev-2o4gzaw5dq-el.a.run.app/question_link/update-status/{id}\"\n",
    "\n",
    "\n",
    "# api = \"https://cloud-trigger-dev-2o4gzaw5dq-el.a.run.app/question_link/update-status/<new>\"\n",
    "# # Making a PUT request\n",
    "r = requests.put(API.format(id = link))\n",
    "print(API.format(id = link))\n",
    "\n",
    "# check status code for response received\n",
    "# success code - 200\n",
    "print(r)\n",
    "\n",
    "# print content of request\n",
    "print(r.content)\n",
    "\n",
    "#61efd3115e85ff38c28daeb3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\"message\":{\"attributes\":{\"urlId\":\"61efd3115e85ff38c28daeb3\",\"inputLink\":\"https://rossbulat.medium.com/using-promises-async-await-with-mongodb-613ed8243900\"}}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = json[\"message\"][\"attributes\"][\"urlId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATABASE_API_ENDPOINT = os.getenv('DATABASE_API_ENDPOINT')\n",
    "\n",
    "print(DATABASE_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeeksforGeeks, is a computer geeks science portal for\n"
     ]
    }
   ],
   "source": [
    "# Python program demonstrating Index error\n",
    "\n",
    "# Number of placeholders are four but\n",
    "# there are only three values passed\n",
    "\n",
    "# parameters in format function.\n",
    "my_string = \"{}, is a {} {} science portal for\"\n",
    "\n",
    "print(my_string.format(\"GeeksforGeeks\", \"computer\", \"geeks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloud-trigger-dev-2o4gzaw5dq-el.a.run.app/question_link/update-status/123\n"
     ]
    }
   ],
   "source": [
    "TEST = 'https://cloud-trigger-dev-2o4gzaw5dq-el.a.run.app'\n",
    "API_ENDPOINT = \"{}/question_link/update-status/{id}\"\n",
    "print(API_ENDPOINT.format(TEST,id=123))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca3dd50bcc18e6c6076b96dc053dc743709d41abddfb6544714a093e6e87823"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('medium': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
